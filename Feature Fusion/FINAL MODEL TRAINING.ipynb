{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FINAL MODEL TRAINING.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFLEzEgRwYZZ","executionInfo":{"status":"ok","timestamp":1646719682625,"user_tz":-330,"elapsed":29795,"user":{"displayName":"Srikandabala Kogul","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ7_wrqRECyV89KjY0M4usgytHh_ss4LSxI_aw3g=s64","userId":"12354510259357034369"}},"outputId":"c115e261-de7d-4e77-a890-a1e023ec58b7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **1) IMPORTING LIBRARIES AND DEFINING FUNCTIONS**\n","\n","<ins>**Functions**</ins>\n","<br>\n","<br>\n","The following table defines and describes the functions used throughout this notebook.\n","<br>\n","<br>\n","\n","\\begin{array}{|c|c|} \\hline\n","\\textbf{Function} & \\textbf{Description} \\\\ \\hline\n","\\textbf{Convert Label} & Converts\\hspace{0.15cm}all\\hspace{0.15cm}types\\hspace{0.15cm}of\\hspace{0.15cm}true\\hspace{0.15cm}news\\hspace{0.15cm}to\\hspace{0.15cm}0\\hspace{0.15cm}and\\hspace{0.15cm}fake\\hspace{0.15cm}to\\hspace{0.15cm}1\\hspace{0.15cm} \\\\ \\hline\n","\\textbf{Get Fake Score, Get True Score} & Get\\hspace{0.15cm}respective\\hspace{0.15cm}scores\\hspace{0.15cm}from\\hspace{0.15cm}array \\\\ \\hline\n","\\textbf{Get Scaler} & Returns\\hspace{0.15cm}the\\hspace{0.15cm}scaler\\hspace{0.15cm}object\\hspace{0.15cm}to\\hspace{0.15cm}transform\\hspace{0.15cm}data \\\\  \\hline\n","\\textbf{Get All Data} &  \\\\ \\hline\n","\\textbf{Evaluate Model} & Given\\hspace{0.15cm}data\\hspace{0.15cm}and\\hspace{0.15cm}model\\hspace{0.15cm}returns\\hspace{0.15cm}accuracy,\\hspace{0.15cm}f1,\\hspace{0.15cm}recall\\hspace{0.15cm}and\\hspace{0.15cm}precision \\\\ \\hline\n","\\textbf{Get Dataset} & Given\\hspace{0.15cm}dataset\\hspace{0.15cm}name\\hspace{0.15cm}returns\\hspace{0.15cm}X\\_train,\\hspace{0.15cm}y\\_train,\\hspace{0.15cm}X\\_test\\hspace{0.15cm}and\\hspace{0.15cm}y\\_test \\\\ \\hline\n","\\textbf{Get Dataset One vs Rest} & Given\\hspace{0.15cm}dataset\\hspace{0.15cm}name\\hspace{0.15cm}returns\\hspace{0.15cm}X\\_train\\hspace{0.15cm}and\\hspace{0.15cm}y\\_train\\hspace{0.15cm}from\\hspace{0.15cm}it\\hspace{0.15cm}and\\hspace{0.15cm}X\\_test\\hspace{0.15cm}and\\hspace{0.15cm}y\\_test\\hspace{0.15cm}from\\hspace{0.15cm}the\\hspace{0.15cm}rest\\hspace{0.15cm} \\\\ \\hline\n","\\end{array}\n","\n","<br>"],"metadata":{"id":"2BeYbI_IbZz4"}},{"cell_type":"code","source":["! pip install mlens -q\n","! pip install deslib -q"],"metadata":{"id":"HsOAGUDwbybG","executionInfo":{"status":"ok","timestamp":1646719690834,"user_tz":-330,"elapsed":8232,"user":{"displayName":"Srikandabala Kogul","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ7_wrqRECyV89KjY0M4usgytHh_ss4LSxI_aw3g=s64","userId":"12354510259357034369"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6727fc88-ce66-47ba-a91c-20d28134f1a2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 227 kB 3.2 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 3.3 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["import pickle\n","\n","# read and write model to pickle\n","def write_to_pickle(path,model):\n","  with open(path, 'wb') as file:  \n","      pickle.dump(model, file)\n","\n","def read_pickle_model(path):\n","  with open(path, 'rb') as file:  \n","      return pickle.load(file)"],"metadata":{"id":"TAp7R8Glc_Rc","executionInfo":{"status":"ok","timestamp":1646719690835,"user_tz":-330,"elapsed":34,"user":{"displayName":"Srikandabala Kogul","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ7_wrqRECyV89KjY0M4usgytHh_ss4LSxI_aw3g=s64","userId":"12354510259357034369"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ============================================================================================================================================= #\n","# SEMANTIC_FEATURES = [\"fake_score\", \"true_score\", \"embd_true\",\"embd_fake\"]\n","SEMANTIC_FEATURES = [\"fake_score_occ\", \"true_score_occ\", \"fake_score_doc\", \"true_score_doc\", \"embd_true\",\"embd_fake\"]\n","EMOTION_FEATURES  = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n","LANGUAGE_FEATURES = ['words_per_sentence', 'characters_per_word', 'punctuations_per_sentence', 'get_sentiment_polarity', \n","                    'lexical_diversity', 'content_word_diversity', 'redundancy', 'noun', 'verb', 'adj', 'adv', \"qn_symbol_per_sentence\", \n","                    \"num_exclamation_per_sentence\", \"url_count_per_sentence\"]\n","\n","# \"fake_score_occ\", \"true_score_occ\", \"fake_score_doc\", \"true_score_doc\"\n","\n","ALL_FEATURES = SEMANTIC_FEATURES + EMOTION_FEATURES + LANGUAGE_FEATURES\n","# ============================================================================================================================================= #\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import PowerTransformer\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import QuantileTransformer\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","import xgboost \n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from lightgbm import LGBMClassifier\n","\n","from deslib.des.knora_e import KNORAE\n","from deslib.des import METADES\n","from deslib.dcs.mcb import MCB\n","from mlens.ensemble import SuperLearner\n","\n","import numpy as np\n","rng = np.random.RandomState(42)\n","import pandas as pd\n","from google.colab import data_table\n","data_table.enable_dataframe_formatter()\n","# ============================================================================================================================================= #\n","def convert_label(label):\n","\n","  if label in ['true', 'mostly-true', 'half-true', 'real', 'Real', 0, 'REAL']:\n","    return 0\n","\n","  if label in ['false', 'pants-fire', 'barely-true', 'fake', 'Fake', 1, 'FAKE']:\n","    return 1\n","# ============================================================================================================================================= #\n","def get_fake_score(scores):\n","  return float(scores.strip(\"[]\").split(\",\")[0].strip())\n","\n","def get_true_score(scores):\n","  return float(scores.strip(\"[]\").split(\",\")[1].strip())\n","\n","def get_fake_score_occ(scores):\n","  return float(scores.strip(\"[]\").split(\",\")[2].strip())\n","\n","def get_fake_score_doc(scores):\n","  return float(scores.strip(\"[]\").split(\",\")[3].strip())\n","\n","def get_true_score_occ(scores):\n","  return float(scores.strip(\"[]\").split(\",\")[4].strip())\n","\n","def get_true_score_doc(scores):\n","  return float(scores.strip(\"[]\").split(\",\")[5].strip())\n","# ============================================================================================================================================= #\n","def get_scaler(X, type_of_scaler=\"standard\"):\n","  \n","  if type_of_scaler == \"get_all\":\n","    scaler = {}\n","    scaler[\"standard\"] = StandardScaler().fit(X)\n","    scaler[\"power\"] = PowerTransformer().fit(X)\n","    scaler[\"robust\"] = RobustScaler().fit(X)\n","    scaler[\"quantile\"] = QuantileTransformer(random_state=0).fit(X)\n","    scaler[\"normalizer\"] = Normalizer().fit(X)\n","    scaler[\"minmax\"] = MinMaxScaler.fit(X)\n","  else: \n","    if type_of_scaler == \"standard\":\n","      scaler = StandardScaler().fit(X)\n","    if type_of_scaler == \"power\":\n","      scaler = PowerTransformer().fit(X)\n","    if type_of_scaler == \"robust\":\n","      scaler = RobustScaler().fit(X)\n","    if type_of_scaler == \"quantile\":\n","      scaler = QuantileTransformer(random_state=0).fit(X)\n","    if type_of_scaler == \"normalizer\":\n","      scaler = Normalizer().fit(X)\n","    if type_of_scaler == \"minmax\":\n","      scaler = MinMaxScaler().fit(X)\n","\n","  return scaler\n","# ============================================================================================================================================= #\n","def get_all_data(datasets):\n","\n","  X = []\n","  \n","  for dataset in datasets:\n","    df = data[dataset_name][\"combined_dataframe\"]\n","    X_temp = np.array(df[ALL_FEATURES]).tolist()\n","    X.extend(X_temp)\n","\n","  return np.array(X)\n","# ============================================================================================================================================= #\n","def evaluate_model(X_train, y_train, X_test, y_test, model_name):\n","\n","  if model_name == 'lgr':\n","    model = LogisticRegression(random_state=0).fit(X_train, y_train)\n","  if model_name == 'dtc':\n","    model = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n","  if model_name == 'gnb':\n","    model = GaussianNB().fit(X_train, y_train)\n","  if model_name == 'rfc':\n","    model = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n","  if model_name == 'knc':\n","    model = KNeighborsClassifier().fit(X_train, y_train)\n","  if model_name == 'abc':\n","    model = AdaBoostClassifier(random_state=0).fit(X_train, y_train)\n","  if model_name == 'xgb':\n","    model = xgboost.XGBClassifier().fit(X_train, y_train)\n","  if model_name == 'etc':\n","    model = ExtraTreesClassifier(random_state=0).fit(X_train, y_train)\n","  if model_name == 'bgc':\n","    model = BaggingClassifier(random_state=0).fit(X_train, y_train)\n","  if model_name == 'lgm':\n","    model = LGBMClassifier().fit(X_train, y_train)\n","\n","  y_pred = model.predict(X_test)\n","\n","  return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average=\"micro\"), f1_score(y_test, y_pred, average=\"macro\"), \\\n","          precision_score(y_test, y_pred, average=\"micro\"), precision_score(y_test, y_pred, average=\"macro\"), \\\n","          recall_score(y_test, y_pred, average=\"micro\"), recall_score(y_test, y_pred, average=\"macro\")\n","# ============================================================================================================================================= #\n","def get_dataset(dataset_name, data):\n","  \n","  df = data[dataset_name][\"combined_dataframe\"]\n","  test_split_available = False\n","\n","  try:\n","    \n","    df_train = df.loc[df['split'] != 'test']\n","    df_test = df.loc[df['split'] == 'test']\n","    test_split_available = True\n","  except KeyError:\n","    pass\n","\n","  if(test_split_available):\n","\n","    X_train, y_train = df_train[ALL_FEATURES].to_numpy(), df_train[['label']].to_numpy()\n","\n","    X_test, y_test = df_test[ALL_FEATURES].to_numpy(), df_test[['label']].to_numpy()\n","\n","  else:\n","\n","    from sklearn.model_selection import train_test_split\n","    X, y = df[ALL_FEATURES].to_numpy(), df[['label']].to_numpy()\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n","\n","  \n","  return X_train, y_train, X_test, y_test\n","# ============================================================================================================================================= #\n","def get_dataset_one_v_rest(dataset_name, data):\n","  \n","  df_train = data[dataset_name][\"combined_dataframe\"]\n","  X_train, y_train = df_train[ALL_FEATURES].to_numpy(), df_train[['label']].to_numpy()\n","\n","  X_test, y_test = [], []\n","\n","  for d in data:\n","\n","      if d != dataset_name:\n","\n","        df_test = data[d][\"combined_dataframe\"]\n","        X, y = np.array(df[ALL_FEATURES]).tolist(), np.array(df[['label']]).tolist()\n","\n","        X_test.extend(X)\n","        y_test.extend(y)\n","    \n","  X_test, y_test = np.array(X_test), np.array(y_test)\n","\n","  return X_train, y_train, X_test, y_test\n","# ============================================================================================================================================= #\n","def get_emotion_language_semantic(dataset_name, data):\n","\n","  df = data[dataset_name][\"combined_dataframe\"]\n","\n","  X_emotion = df[EMOTION_FEATURES].to_numpy()\n","  X_language = df[LANGUAGE_FEATURES].to_numpy()\n","  X_semantic = df[SEMANTIC_FEATURES].to_numpy()\n","  \n","  y = df['label'].to_numpy()\n","\n","  return X_emotion, X_language, X_semantic, y\n","# ============================================================================================================================================= #\n","def get_emotion_language_semantic_test(dataset_name, data):\n","\n","  df = data[dataset_name][\"combined_dataframe\"]\n","\n","  try:\n","    df = df.loc[df['split'] == 'test']\n","  except KeyError:\n","    pass\n","  \n","  X_emotion = df[EMOTION_FEATURES].to_numpy()\n","  X_language = df[LANGUAGE_FEATURES].to_numpy()\n","  X_semantic = df[SEMANTIC_FEATURES].to_numpy()\n","  \n","  y = df['label'].to_numpy()\n","\n","  return X_emotion, X_language, X_semantic, y\n","# ============================================================================================================================================= #\n","def get_dataset_all(dataset_name, data):\n","\n","  df = data[dataset_name][\"combined_dataframe\"]\n","\n","  X = df[ALL_FEATURES].to_numpy()\n","\n","  y = df['label'].to_numpy()\n","\n","  return X, y\n","# ============================================================================================================================================= #\n","def shape_outliers(dataFrame, features):\n","\n","  dataframe = dataFrame.copy(deep=True)\n","\n","  for column in dataframe[features].columns.tolist():\n","\n","    Q1 = dataframe[column].quantile(0.25)\n","    Q3 = dataframe[column].quantile(0.75)\n","    \n","    IQR = (Q3 - Q1)\n","    minV = Q1 - 1.5*IQR\n","    maxV = Q3 + 1.5*IQR\n","    \n","    temp = dataframe[column].copy()\n","\n","    if ( column not in [\"qn_symbol_per_sentence\" , \"num_exclamation_per_sentence\" ,\"lexical_diversity\" ,\"url_count_per_sentence\"] ) :\n","      dataframe[column]=dataframe[column].apply(lambda x:minV if x< minV else maxV if x>maxV else x)\n","\n","      mean = dataframe[column].mean()\n","      std  = dataframe[column].std() \n","\n","      dataframe[column]=dataframe[column].apply(lambda x: (x-mean)/std )\n","      \n","    else:\n","      dataframe[column]=dataframe[column].apply(lambda x : 1 if x>0 else 0)\n","    \n","  return dataframe\n","# ============================================================================================================================================= #"],"metadata":{"id":"I9ohG7VIN3bG","executionInfo":{"status":"ok","timestamp":1646719693049,"user_tz":-330,"elapsed":2246,"user":{"displayName":"Srikandabala Kogul","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ7_wrqRECyV89KjY0M4usgytHh_ss4LSxI_aw3g=s64","userId":"12354510259357034369"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9836661-499c-4cf3-d132-4812ed7f56f6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[MLENS] backend: threading\n"]}]},{"cell_type":"markdown","source":["# **2) IMPORTING DATASETS**\n","\n","<ins>**Datasets Used**</ins>\n","<br>\n","<br>\n","\\begin{array}{|c|c|} \\hline\n","\\textbf{Dataset} & \\textbf{Description} & \\textbf{No. True} & \\textbf{No. Fake}  \\\\ \\hline \\hline\n","\\textbf{LIAR} & News\\hspace{0.15cm}from\\hspace{0.15cm}Politifact & foo & bar \\\\ \\hline\n","\\textbf{CodaLab} & COVID\\hspace{0.15cm}Tweets & foo & bar \\\\ \\hline\n","\\textbf{FakeNewsNet} & Politifact\\hspace{0.15cm}and\\hspace{0.15cm}GossipCop & foo & bar \\\\  \\hline\n","\\textbf{Kaggle} & bar & foo & bar \\\\ \\hline\n","\\textbf{ISOT} & Long\\hspace{0.15cm}text & foo & bar \\\\ \\hline\n","\\end{array}\n","\n","<br>"],"metadata":{"id":"-nl0p2ZXbief"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"BE-75y8HNrOw","executionInfo":{"status":"ok","timestamp":1646719693050,"user_tz":-330,"elapsed":15,"user":{"displayName":"Srikandabala Kogul","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ7_wrqRECyV89KjY0M4usgytHh_ss4LSxI_aw3g=s64","userId":"12354510259357034369"}}},"outputs":[],"source":["datasets = {\n","    \"LIAR\" : {\n","        # \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/LIAR/LIAR_emotion_scores_modified.csv\",\n","        # \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/LIAR/Liar_with_WELFAKE_Lexicon_Scores.csv\",\n","        # \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/LIAR/Liar_sementic.csv\",\n","        # \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/LIAR/LIAR_embedding.csv\"\n","        \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/LIAR/LIAR_emotion_scores_modified.csv\",\n","        \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/LIAR/Liar_with_WELFAKE_Lexicon_Scores_Modified.csv\",\n","        \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/LIAR/LIAR_sementic.csv\",\n","        \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/LIAR/LIAR_embedding_new.csv\"\n","    },\n","\n","    \"ISOT\" : {\n","        # \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/ISOT/ISOT_emotion_scores_modified.csv\",\n","        # \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/ISOT/ISOT_with_WELFAKE_Lexicon_Scores.csv\",\n","        # \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/ISOT/ISOT_sementic.csv\",\n","        # \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/ISOT/ISOT_embedding.csv\"\n","        \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/ISOT/ISOT_emotion_scores_modified.csv\",\n","        \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/ISOT/ISOT_with_WELFAKE_Lexicon_Scores_Modified.csv\",\n","        \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/ISOT/ISOT_sementic.csv\",\n","        \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/ISOT/ISOT_embedding_new.csv\"\n","    },\n","\n","    \"Kaggle_real_fake\" : {\n","        # \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/Kaggle_real_fake/Kaggle_real_fake_emotion_scores_modified.csv\",\n","        # \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/Kaggle_real_fake/Kaggle_real_fake_with_WELFAKE_Lexicon_Scores.csv\",\n","        # \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Kaggle_real_fake/Kaggle_real_fake_sementic.csv\",\n","        # \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/Kaggle_real_fake/Kaggle_real_fake_embedding.csv\" \n","        \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Kaggle_real_fake/Kaggle_real_fake_emotion_scores_modified.csv\",\n","        \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Kaggle_real_fake/Kaggle_real_fake_with_WELFAKE_Lexicon_Scores_Modified.csv\",\n","        \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Kaggle_real_fake/Kaggle_real_fake_sementic.csv\",\n","        \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/Kaggle_real_fake/Kaggle_real_fake_embedding_new.csv\" \n","    },\n","\n","    \"CodaLab\" : {\n","        # \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/CodaLab Covid/Codalab_emotion_scores_modified.csv\",\n","        # \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/CodaLab Covid/CodaLab_with_WELFAKE_Lexicon_Scores.csv\",\n","        # \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/CodaLab Covid/CodaLab_sementic.csv\",\n","        # \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/CodaLab Covid/CodaLab Covid_embedding.csv\"\n","        \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/CodaLab Covid/Codalab_emotion_scores_modified.csv\",\n","        \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/CodaLab Covid/CodaLab_with_WELFAKE_Lexicon_Scores_Modified.csv\",\n","        \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/CodaLab Covid/CodaLab Covid_sementic.csv\",\n","        \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/CodaLab Covid/CodaLab Covid_embedding_new.csv\"\n","    },\n","\n","    \"FakeNewsNet\" : {\n","        # \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/FakeNewsNet/FakeNewsNet_emotion_scores_modified.csv\",\n","        # \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Datasets/FakeNewsNet/FakeNewsNet_with_WELFAKE_Lexicon_Scores.csv\",\n","        # \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/FakeNewsNet/FakeNewsNet_sementic.csv\",\n","        # \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/FakeNewsNet/FakeNewsNet_embedding.csv\"\n","        \"emotion_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/FakeNewsNet/FakeNewsNet_emotion_scores_modified.csv\",\n","        \"lexicon_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/FakeNewsNet/FakeNewsNet_with_WELFAKE_Lexicon_Scores_Modified.csv\",\n","        \"semantic_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/FakeNewsNet/FakeNewsNet_sementic.csv\",\n","        \"embedding_file_path\" : r\"/content/drive/Shareddrives/[FYP] Fake News Detection/Results/Final/FakeNewsNet/FakeNewsNet_embedding_new.csv\"\n","    }\n","    \n","}"]},{"cell_type":"code","source":["for d in datasets:\n","  \n","  print(\"=========================\")\n","  print(\"Reading {}\".format(d))\n","  print(\"=========================\\n\")\n","  \n","  datasets[d][\"lexicon_dataframe\"] = pd.read_csv(datasets[d][\"lexicon_file_path\"]) \n","  datasets[d][\"emotion_dataframe\"] = pd.read_csv(datasets[d][\"emotion_file_path\"]) \n","  datasets[d][\"semantic_dataframe\"] = pd.read_csv(datasets[d][\"semantic_file_path\"])\n","  datasets[d][\"embedding_dataframe\"] = pd.read_csv(datasets[d][\"embedding_file_path\"])\n","\n","  id = {\"CodaLab\":\"id\", \"FakeNewsNet\":\"id_1\", \"ISOT\":\"id\", \"Kaggle_real_fake\":\"id\", \"LIAR\":\"ID\"}\n","  ID = id[d]\n","\n","  df = datasets[d][\"emotion_dataframe\"].merge(datasets[d][\"semantic_dataframe\"], how='inner', on=ID,suffixes=('_Sentiment', '_Semantic'))\n","  df = df.merge(datasets[d][\"lexicon_dataframe\"], how='inner', on = ID,suffixes=('', '_Lexicon'))\n","  df = df.merge(datasets[d][\"embedding_dataframe\"], how='inner', on = ID,suffixes=('', '_Embedding'))\n","\n","  # df[\"fake_score\"], df[\"true_score\"] = df[\"scores\"].apply(get_fake_score), df[\"scores\"].apply(get_true_score)\n","\n","  df[\"fake_score_occ\"], df[\"true_score_occ\"] = df[\"scores\"].apply(get_fake_score_occ), df[\"scores\"].apply(get_true_score_occ)\n","  df[\"fake_score_doc\"], df[\"true_score_doc\"] = df[\"scores\"].apply(get_fake_score_doc), df[\"scores\"].apply(get_true_score_doc)\n","\n","  df = df.loc[df[\"lang\"]==\"en\"].copy(deep=True)\n","\n","  df[\"label\"] = df[\"label\"].apply(convert_label)\n","\n","  df[\"qn_symbol_per_sentence\"] = df[\"qn_symbol\"] / df[\"num_sentences\"]\n","  df[\"num_exclamation_per_sentence\"] = df[\"num_exclamation\"] / df[\"num_sentences\"]\n","  df[\"url_count_per_sentence\"] = df[\"url_count\"] / df[\"num_sentences\"]\n","  \n","  df = shape_outliers(df, ALL_FEATURES)\n","\n","  datasets[d][\"combined_dataframe\"] = df\n","\n","  print(\"Total number of datapoints : {}\\n\".format(len(df)))\n","\n","  del df\n","\n","  print(\"\\nDone !!!\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HX8MZV3BN6QB","executionInfo":{"status":"ok","timestamp":1646719796868,"user_tz":-330,"elapsed":103832,"user":{"displayName":"Srikandabala Kogul","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ7_wrqRECyV89KjY0M4usgytHh_ss4LSxI_aw3g=s64","userId":"12354510259357034369"}},"outputId":"b511de2f-1801-4fb8-94fa-97455a4df6d7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["=========================\n","Reading LIAR\n","=========================\n","\n","Total number of datapoints : 12709\n","\n","\n","Done !!!\n","\n","=========================\n","Reading ISOT\n","=========================\n","\n","Total number of datapoints : 44140\n","\n","\n","Done !!!\n","\n","=========================\n","Reading Kaggle_real_fake\n","=========================\n","\n","Total number of datapoints : 6294\n","\n","\n","Done !!!\n","\n","=========================\n","Reading CodaLab\n","=========================\n","\n","Total number of datapoints : 10620\n","\n","\n","Done !!!\n","\n","=========================\n","Reading FakeNewsNet\n","=========================\n","\n","Total number of datapoints : 21715\n","\n","\n","Done !!!\n","\n"]}]},{"cell_type":"markdown","source":["# **3) RESULTS**\n","\n","| Model Used       | Dataset Tested On | Accuracy    |\n","|------------------|-------------------|-------------|\n","| LIAR             | CodaLab           | 0.503505656 |\n","| LIAR             | Kaggle_real_fake  | 0.508580858 |\n","| LIAR             | FakeNewsNet       | 0.600073651 |\n","| LIAR             | ISOT              | 0.5172832   |\n","| CodaLab          | LIAR              | 0.52972973  |\n","| CodaLab          | Kaggle_real_fake  | 0.541254125 |\n","| CodaLab          | FakeNewsNet       | 0.58198306  |\n","| CodaLab          | ISOT              | 0.456242728 |\n","| Kaggle_real_fake | LIAR              | 0.518292205 |\n","| Kaggle_real_fake | CodaLab           | 0.430214079 |\n","| Kaggle_real_fake | FakeNewsNet       | 0.486190389 |\n","| Kaggle_real_fake | ISOT              | 0.501232129 |\n","| FakeNewsNet      | LIAR              | 0.555111633 |\n","| FakeNewsNet      | CodaLab           | 0.53398149  |\n","| FakeNewsNet      | Kaggle_real_fake  | 0.507920792 |\n","| FakeNewsNet      | ISOT              | 0.502954649 |\n","| ISOT             | LIAR              | 0.550724638 |\n","| ISOT             | CodaLab           | 0.524726559 |\n","| ISOT             | Kaggle_real_fake  | 0.50709571  |\n","| ISOT             | FakeNewsNet       | 0.723899834 |\n"],"metadata":{"id":"nqAnq9gJbnq9"}},{"cell_type":"code","source":["classifiers = {\n","    \"LIAR\" : {},\n","    \"CodaLab\" : {},\n","    \"Kaggle_real_fake\" : {},\n","    \"FakeNewsNet\" : {},\n","    \"ISOT\" : {}\n","}\n","\n","test_size = 0.5\n","\n","for dataset in classifiers:\n","\n","  # print(\"Training Extra tree classifier\")\n","  # print(\"------------------------------\")\n","  # print()\n","\n","  # X, y = get_dataset_all(dataset, datasets)\n","\n","  # scaler = get_scaler(X, \"standard\")\n","  # scaler.transform(X)\n","\n","  # model = ExtraTreesClassifier().fit(X, y.ravel())\n","  # classifiers[dataset][\"model\"] = model\n","  # classifiers[dataset][\"scaler\"] = scaler\n","\n","  # print(\"\\nDone !!! \\n\")\n","# ============================================================================================================================================= #\n","  # print(\"==================================\")\n","  # print(\"Training {}\".format(dataset))\n","  # print(\"==================================\")\n","  # print()\n","\n","  # X_emotion, X_language, X_semantic, y = get_emotion_language_semantic(dataset, datasets)\n","\n","  # print(\"Training Emotion\")\n","  # print(\"----------------\")\n","  # model = ExtraTreesClassifier(n_estimators=300).fit(X_emotion, y.ravel())\n","  # file_path = \"{}_emotion_model.pkl\".format(dataset)\n","  # write_to_pickle(file_path, model)\n","  # classifiers[dataset][\"emotion_model\"] = file_path\n","  # del model\n","  # print()\n","\n","  # print(\"Training Language\")\n","  # print(\"-----------------\")\n","  # model = ExtraTreesClassifier(n_estimators=300).fit(X_language, y.ravel())\n","  # file_path = \"{}_language_model.pkl\".format(dataset)\n","  # write_to_pickle(file_path, model)\n","  # classifiers[dataset][\"language_model\"] = file_path\n","  # del model\n","  # print()\n","\n","  # print(\"Training Semantic\")\n","  # print(\"-----------------\")\n","  # model = ExtraTreesClassifier(n_estimators=300).fit(X_semantic, y.ravel())\n","  # file_path = \"{}_semantic_model.pkl\".format(dataset)\n","  # write_to_pickle(file_path, model)\n","  # classifiers[dataset][\"semantic_model\"] = file_path\n","  # del model\n","  # print()\n","\n","  # del X_emotion\n","  # del X_language\n","  # del X_semantic\n","  # del y\n","\n","#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#\n","\n","  print(\"==================================\")\n","  print(\"Training {}\".format(dataset))\n","  print(\"==================================\")\n","  print()\n","\n","  X_emotion, X_language, X_semantic, y = get_emotion_language_semantic(dataset, datasets)\n","  \n","  print(\"Training Emotion\")\n","  print(\"----------------\")\n","  X_train, X_dsel, y_train, y_dsel = train_test_split(X_emotion, y, test_size=test_size, random_state=0, stratify=y)\n","  print(\"- Training Extra Tree Classifier\")\n","  pool_classifiers = ExtraTreesClassifier(n_estimators=1000).fit(X_train, y_train.ravel())\n","  print(\"- Training DES\")\n","  model = METADES(pool_classifiers = pool_classifiers, k = 20)\n","  model.fit(X_dsel, y_dsel.ravel())\n","  file_path = \"{}_emotion_model.pkl\".format(dataset)\n","  write_to_pickle(file_path, model)\n","  classifiers[dataset][\"emotion_model\"] = file_path\n","  del pool_classifiers\n","  del model\n","  print()\n","\n","  print(\"Training Language\")\n","  print(\"-----------------\")\n","  X_train, X_dsel, y_train, y_dsel = train_test_split(X_language, y, test_size=test_size, random_state=0, stratify=y)\n","  print(\"- Training Extra Tree Classifier\")\n","  pool_classifiers = ExtraTreesClassifier(n_estimators=1000).fit(X_train, y_train.ravel())\n","  print(\"- Training DES\")\n","  model = METADES(pool_classifiers = pool_classifiers, k = 20)\n","  model.fit(X_dsel, y_dsel.ravel())\n","  file_path = \"{}_language_model.pkl\".format(dataset)\n","  write_to_pickle(file_path, model)\n","  classifiers[dataset][\"language_model\"] = file_path\n","  del pool_classifiers\n","  del model\n","  print()\n","\n","  print(\"Training Semantic\")\n","  print(\"-----------------\")\n","  X_train, X_dsel, y_train, y_dsel = train_test_split(X_semantic, y, test_size=test_size, random_state=0, stratify=y)\n","  print(\"- Training Extra Tree Classifier\")\n","  pool_classifiers = ExtraTreesClassifier(n_estimators=1000).fit(X_train, y_train.ravel())\n","  print(\"- Training DES\")\n","  model = METADES(pool_classifiers = pool_classifiers, k = 20)\n","  model.fit(X_dsel, y_dsel.ravel())\n","  file_path = \"{}_semantic_model.pkl\".format(dataset)\n","  write_to_pickle(file_path, model)\n","  classifiers[dataset][\"semantic_model\"] = file_path\n","  del pool_classifiers\n","  del model\n","  print()\n","\n","  del X_train\n","  del X_dsel\n","  del y_train\n","  del y_dsel\n","\n","  print(\"\\nDone !!!\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SiHBy4SioK3K","outputId":"95d55f26-678f-4049-eb3c-f53f30ecddeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================\n","Training LIAR\n","==================================\n","\n","Training Emotion\n","----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","Training Language\n","-----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","Training Semantic\n","-----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","\n","Done !!!\n","\n","==================================\n","Training CodaLab\n","==================================\n","\n","Training Emotion\n","----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","Training Language\n","-----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","Training Semantic\n","-----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","\n","Done !!!\n","\n","==================================\n","Training Kaggle_real_fake\n","==================================\n","\n","Training Emotion\n","----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","Training Language\n","-----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","Training Semantic\n","-----------------\n","- Training Extra Tree Classifier\n","- Training DES\n","\n","\n","Done !!!\n","\n","==================================\n","Training FakeNewsNet\n","==================================\n","\n","Training Emotion\n","----------------\n","- Training Extra Tree Classifier\n","- Training DES\n"]}]},{"cell_type":"code","source":["from scipy.stats import mode\n","\n","all_datasets = list(classifiers.keys())\n","which_model, which_dataset, accuracies = [], [], []\n","\n","for data in classifiers:\n","  print(\"========================================\")\n","  print(\"Testing Model Trained on {}\".format(data))\n","  print(\"========================================\")\n","  print()\n","\n","  total_accuracy = 0\n","\n","  for d in all_datasets:\n","    \n","    if d == data:\n","      continue \n","\n","    #======================================================================================#\n","\n","    # X_test, y_test = get_dataset_all(d, datasets)\n","    # X_test = classifiers[data][\"scaler\"].transform(X_test)\n","    # y_pred = classifiers[data][\"model\"].predict(X_test)\n","\n","    #======================================================================================#\n","    X_emotion, X_language, X_semantic, y_test = get_emotion_language_semantic_test(d, datasets)\n","    # X_semantic = classifiers[data][\"semantic_scaler\"].transform(X_semantic)\n","    # X_emotion = classifiers[data][\"emotion_scaler\"].transform(X_emotion)\n","    # X_language = classifiers[data][\"language_scaler\"].transform(X_language)\n","\n","    model = read_pickle_model(classifiers[data][\"emotion_model\"])\n","    y_pred_emotion = model.predict(X_emotion)\n","\n","    model = read_pickle_model(classifiers[data][\"language_model\"])\n","    y_pred_language = model.predict(X_language)\n","\n","    model = read_pickle_model(classifiers[data][\"semantic_model\"])\n","    y_pred_semantic = model.predict(X_semantic)\n","\n","    y_pred = np.vstack((y_pred_emotion, y_pred_language, y_pred_semantic))\n","    y_pred = mode(y_pred)[0][0]\n","    #======================================================================================#\n","\n","    print(\"Accuracy on {} is {}\\n\".format(d, accuracy_score(y_pred, y_test)))\n","    total_accuracy += accuracy_score(y_pred, y_test)\n","    which_model.append(data)\n","    which_dataset.append(d)\n","    accuracies.append(accuracy_score(y_pred, y_test))\n","    \n","    del model\n","\n","  print(\"Average Accuracy Across All datasets is {}\".format(total_accuracy/(len(all_datasets)-1)))\n","  print()"],"metadata":{"id":"nngGaa5euERB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_final = pd.DataFrame()\n","results_final[\"Model Used\"] = which_model\n","results_final[\"Dataset Tested On\"] = which_dataset\n","results_final[\"Accuracy\"] = accuracies"],"metadata":{"id":"cFAVrZ-Qx2A7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_final"],"metadata":{"id":"jkGZnDQaDLvb"},"execution_count":null,"outputs":[]}]}